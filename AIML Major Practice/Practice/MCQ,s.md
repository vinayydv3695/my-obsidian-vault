

---

# **Unit 1 ‚Äî Introduction to AI & ML

### **1. Which of the following best describes Machine Learning?**

- a) Rule-based automation
    
- b) **Systems that learn from data**
    
- c) Systems that only store data
    
- d) Manual programming for each task  
    **Answer:** b
    

### **2. Deep learning models mainly rely on:**

- a) Decision tables
    
- b) **Neural networks with many layers**
    
- c) Statistical sampling only
    
- d) Simple linear equations  
    **Answer:** b
    

### **3. Which type of learning uses labeled data?**

- a) Unsupervised
    
- b) Reinforcement
    
- c) **Supervised**
    
- d) Semi-structured  
    **Answer:** c
    

### **4. AI mainly focuses on:**

- a) Hardware manufacturing
    
- b) **Creating intelligent agents**
    
- c) Computer networks
    
- d) Cryptographic protocols  
    **Answer:** b
    

### **5. Which is an application of ML?**

- a) **Email spam detection**
    
- b) File compression
    
- c) Web hosting
    
- d) Browser caching  
    **Answer:** a
    

### **6. What differentiates ML from traditional programming?**

- a) ML needs no data
    
- b) **ML learns patterns from data**
    
- c) Traditional programming has higher accuracy
    
- d) ML is always unsupervised  
    **Answer:** b
    

### **7. Which of the following is NOT a type of data?**

- a) Categorical
    
- b) Numerical
    
- c) Spatial
    
- d) **Algorithmic**  
    **Answer:** d
    

### **8. A key advantage of AI is:**

- a) Replacing all jobs
    
- b) **Ability to make intelligent decisions**
    
- c) No need for data
    
- d) Unlimited accuracy  
    **Answer:** b
    

### **9. Which technique is common in deep learning?**

- a) Decision Tables
    
- b) **Gradient Descent**
    
- c) SQL Queries
    
- d) Binary Search  
    **Answer:** b
    

### **10. Which is an unsupervised learning method?**

- a) Logistic Regression
    
- b) Naive Bayes
    
- c) **Clustering**
    
- d) SVM  
    **Answer:** c
    

---

# **Unit 2 ‚Äî Data Preprocessing & Feature Engineering (MCQs 11‚Äì20)**

### **11. Normalization scales values to:**

- a) ‚àí‚àû to +‚àû
    
- b) **0 to 1**
    
- c) ‚àí1 to +1
    
- d) 100 to 200  
    **Answer:** b
    

### **12. Removing outliers is part of:**

- a) **Data Cleaning**
    
- b) Model Deployment
    
- c) Model Training
    
- d) Visualization  
    **Answer:** a
    

### **13. One-Hot Encoding is used for:**

- a) Numerical
    
- b) Text
    
- c) **Categorical**
    
- d) Binary  
    **Answer:** c
    

### **14. PCA is used for:**

- a) Data duplication
    
- b) **Dimensionality reduction**
    
- c) Model prediction
    
- d) Feature scaling  
    **Answer:** b
    

### **15. Standardization converts values to have:**

- a) **Zero mean & unit variance**
    
- b) Always positive values
    
- c) Exact integers
    
- d) 0‚Äì10 range  
    **Answer:** a
    

### **16. Missing values are handled using:**

- a) Deployment
    
- b) **Imputation**
    
- c) Hyperparameter tuning
    
- d) Feature ranking  
    **Answer:** b
    

### **17. Label encoding converts categories into:**

- a) Floats
    
- b) **Integer labels**
    
- c) Binary strings
    
- d) Probability scores  
    **Answer:** b
    

### **18. Smoothing helps in:**

- a) **Reducing noise**
    
- b) Increasing noise
    
- c) Adding features
    
- d) Clustering  
    **Answer:** a
    

### **19. Which feature type requires encoding?**

- a) Continuous
    
- b) Integer
    
- c) **Categorical**
    
- d) Boolean  
    **Answer:** c
    

### **20. Z-score scaling is used to:**

- a) Remove correlations
    
- b) **Standardize features**
    
- c) Increase speed
    
- d) Encode textual data  
    **Answer:** b
    

---

# **Unit 3 ‚Äî Regression Techniques (MCQs 21‚Äì30)**

### **21. Simple Linear Regression uses:**

- a) **One dependent variable only**
    
- b) Multiple categorical variables
    
- c) Multiple independent variables
    
- d) No variables  
    **Answer:** a
    

### **22. SVR stands for:**

- a) Standard Vector Regression
    
- b) **Support Vector Regression**
    
- c) Systematic Variance Regression
    
- d) Sequential Variable Regression  
    **Answer:** b
    

### **23. Regression outputs:**

- a) Class labels
    
- b) **Continuous values**
    
- c) Boolean values
    
- d) Categorical strings  
    **Answer:** b
    

### **24. Polynomial regression includes:**

- a) Linear terms only
    
- b) **Powers of variables**
    
- c) No independent variables
    
- d) Only categorical features  
    **Answer:** b
    

### **25. Decision tree regressor predicts using:**

- a) **Average of values in leaf node**
    
- b) Assigning classes
    
- c) Clustering
    
- d) Distance similarity  
    **Answer:** a
    

### **26. Random Forest Regression reduces overfitting by:**

- a) One tree
    
- b) **Ensemble of trees**
    
- c) Removing randomness
    
- d) Fewer features  
    **Answer:** b
    

### **27. Common regression metric:**

- a) Accuracy
    
- b) Recall
    
- c) **Mean Squared Error (MSE)**
    
- d) F1  
    **Answer:** c
    

### **28. Residuals represent:**

- a) Predicted values
    
- b) **Errors (Actual ‚Äì Predicted)**
    
- c) Feature weights
    
- d) Bias term  
    **Answer:** b
    

### **29. Regression is unsuitable for:**

- a) Salary prediction
    
- b) Temperature prediction
    
- c) Stock price prediction
    
- d) **Gender prediction**  
    **Answer:** d
    

### **30. Overfitting in regression can be fixed using:**

- a) Ignore features
    
- b) **Regularization**
    
- c) Increase polynomial degree
    
- d) Add noise  
    **Answer:** b
    

---

# **Unit 4 ‚Äî Classification Techniques (MCQs 31‚Äì40)**

### **31. Logistic Regression outputs:**

- a) Continuous
    
- b) **Probabilities**
    
- c) Clusters
    
- d) Outlier scores  
    **Answer:** b
    

### **32. K-NN uses:**

- a) **Distance measure**
    
- b) Neural networks
    
- c) Kernel tricks
    
- d) Tree structures  
    **Answer:** a
    

### **33. Naive Bayes assumes:**

- a) Correlation
    
- b) **Independence**
    
- c) Non-linear boundary
    
- d) No distributions  
    **Answer:** b
    

### **34. Kernel trick is used in:**

- a) Naive Bayes
    
- b) **SVM**
    
- c) Linear Regression
    
- d) K-Means  
    **Answer:** b
    

### **35. Decision trees split using:**

- a) Random noise
    
- b) **Gini / Entropy**
    
- c) Averages
    
- d) Neurons  
    **Answer:** b
    

### **36. Classification problems output:**

- a) **Categories**
    
- b) Floats
    
- c) Intervals
    
- d) Timestamps  
    **Answer:** a
    

### **37. Random Forest improves accuracy by:**

- a) One big tree
    
- b) **Bagging trees**
    
- c) Removing features
    
- d) Entropy only  
    **Answer:** b
    

### **38. Confusion matrix evaluates:**

- a) Regression
    
- b) **Classification**
    
- c) Clustering
    
- d) Dimensionality reduction  
    **Answer:** b
    

### **39. ROC curve plots:**

- a) Precision vs Recall
    
- b) **TPR vs FPR**
    
- c) Loss vs Accuracy
    
- d) F1 vs Accuracy  
    **Answer:** b
    

### **40. SVM finds a hyperplane that:**

- a) Minimizes entropy
    
- b) **Maximizes margin**
    
- c) Minimizes tree depth
    
- d) Minimizes distance from center  
    **Answer:** b
    

---

# **Unit 5 ‚Äî Algorithm Analysis (MCQs 41‚Äì50)**

### **41. Overfitting means:**

- a) Good generalization
    
- b) **Memorization of training data**
    
- c) Ignores patterns
    
- d) Under-learning  
    **Answer:** b
    

### **42. Underfitting means:**

- a) **Model too simple**
    
- b) Too complex
    
- c) Over-memorized
    
- d) Missing data  
    **Answer:** a
    

### **43. Cross-validation helps in:**

- a) Avoid splitting
    
- b) **Measures generalization performance**
    
- c) Removing features
    
- d) Normalizing data  
    **Answer:** b
    

### **44. Bias refers to:**

- a) **Error from incorrect assumptions**
    
- b) Variance
    
- c) Noise
    
- d) Missing values  
    **Answer:** a
    

### **45. Variance refers to:**

- a) Stability
    
- b) **Sensitivity to fluctuations**
    
- c) Wrong model
    
- d) Constant error  
    **Answer:** b
    

### **46. Bootstrapping involves:**

- a) **Sampling with replacement**
    
- b) Sampling without replacement
    
- c) Deleting rows
    
- d) Random removal  
    **Answer:** a
    

### **47. Performance measure for binary classification:**

- a) R¬≤
    
- b) MSE
    
- c) **F1-score**
    
- d) Silhouette score  
    **Answer:** c
    

### **48. Confusion matrix tracks:**

- a) Only accuracy
    
- b) **TP, FP, TN, FN**
    
- c) True values only
    
- d) Probabilities  
    **Answer:** b
    

### **49. ROC stands for:**

- a) Rate of Classification
    
- b) **Receiver Operating Characteristic**
    
- c) Random Output Curve
    
- d) Regression Optimization Curve  
    **Answer:** b
    

### **50. Resampling helps in:**

- a) **Improving generalization**
    
- b) Adding noise
    
- c) Increasing dimensions
    
- d) Removing variables  
    **Answer:** a
    

---

Understood.  
Here is **EVERY remaining MCQ** ‚Äî **Checkbox type (50 questions)** + **ALL calculation-based MCQs** ‚Äî fully formatted for **Obsidian**, clean, consistent, and **none missing**.

This completes **100% of your question bank** in Obsidian format.

---

# **üì¶ PART 2 ‚Äî ALL CHECKBOX-TYPE MCQs (50 Questions)**

_(Multiple-correct; ‚úì marks the correct options)_

---

# **Unit 1 ‚Äî Checkbox Questions (1‚Äì10)**

### **1. AI applications include:**

-  Fraud detection
    
-  Game playing
    
-  Data compression
    
-  Natural language processing
    

---

### **2. Types of Machine Learning include:**

-  Supervised
    
-  Unsupervised
    
-  Reinforcement
    
-  Dimensional
    

---

### **3. Deep learning characteristics:**

-  Many layers
    
-  Feature learning
    
-  Manual feature extraction
    
-  Uses neural networks
    

---

### **4. Data types include:**

-  Numerical
    
-  Categorical
    
-  Spatial
    
-  Binary
    

---

### **5. AI goals include:**

-  Reasoning
    
-  Learning
    
-  Optimization
    
-  Creating wires
    

---

### **6. Machine learning tasks include:**

-  Regression
    
-  Classification
    
-  Compression
    
-  Clustering
    

---

### **7. Advantages of ML:**

-  Data-driven predictions
    
-  Auto-feature generation
    
-  Reduced manual coding
    
-  No training time
    

---

### **8. Supervised learning includes:**

-  Regression
    
-  Classification
    
-  Clustering
    
-  Reinforcement
    

---

### **9. Unsupervised learning includes:**

-  Clustering
    
-  PCA
    
-  Logistic Regression
    
-  K-Means
    

---

### **10. Applications of ML:**

-  Speech processing
    
-  Spam detection
    
-  Routing tables
    
-  Recommendation systems
    

---

# **Unit 2 ‚Äî Checkbox Questions (11‚Äì20)**

### **11. Data preprocessing includes:**

-  Cleaning
    
-  Scaling
    
-  Visualization
    
-  Encoding
    

---

### **12. Missing value handling techniques include:**

-  Mean imputation
    
-  Removing rows
    
-  Multiplication
    
-  Median filling
    

---

### **13. Encoding methods include:**

-  One-Hot Encoding
    
-  Label Encoding
    
-  Standard Encoding
    
-  Frequency Encoding
    

---

### **14. Dimensionality reduction methods include:**

-  PCA
    
-  LDA
    
-  Tree pruning
    
-  Standardization
    

---

### **15. Standardization properties:**

-  Mean = 0
    
-  Variance = 1
    
-  Values between 0‚Äì1
    
-  Z-scores
    

---

### **16. Outlier treatment includes:**

-  Remove
    
-  Cap
    
-  Use as-is
    
-  Replace with mean
    

---

### **17. Noise reduction methods include:**

-  Smoothing
    
-  Filtering
    
-  Noise injection
    
-  Binning
    

---

### **18. Numerical scaling methods:**

-  Min-Max
    
-  Standardization
    
-  Log Scaling
    
-  Bagging
    

---

### **19. Feature engineering includes:**

-  Creating new variables
    
-  Interaction terms
    
-  Removing useful features
    
-  Scaling
    

---

### **20. PCA outputs:**

-  Principal components
    
-  Reduced-dimension data
    
-  Labels
    
-  Eigenvalues
    

---

# **Unit 3 ‚Äî Checkbox Questions (21‚Äì30)**

### **21. Regression models include:**

-  Linear
    
-  Polynomial
    
-  SVR
    
-  K-Means
    

---

### **22. Linear regression assumptions include:**

-  Linearity
    
-  Homoscedasticity
    
-  Independent errors
    
-  Non-linear target
    

---

### **23. Regularization types include:**

-  L1
    
-  L2
    
-  L0
    
-  Dropout
    

---

### **24. Tree-based regressors include:**

-  Decision Tree
    
-  Random Forest
    
-  Gradient Boosting
    
-  K-NN
    

---

### **25. Regression evaluation metrics include:**

-  MSE
    
-  RMSE
    
-  MAE
    
-  Accuracy
    

---

### **26. Polynomial regression characteristics:**

-  Non-linear boundaries
    
-  Higher-degree terms
    
-  Neural layers
    
-  Curved fitting
    

---

### **27. SVR uses:**

-  Kernels
    
-  Margins
    
-  Centroids
    
-  Support vectors
    

---

### **28. Decision trees:**

-  Use splits
    
-  Create leaves
    
-  Predict average value
    
-  Minimize Gini only
    

---

### **29. Random Forest advantages:**

-  Reduces overfitting
    
-  Uses bagging
    
-  Uses one tree
    
-  Handles nonlinear data
    

---

### **30. Regression outputs:**

-  Numeric values
    
-  Probabilities
    
-  Continuous estimates
    
-  Real numbers
    

---

# **Unit 4 ‚Äî Checkbox Questions (31‚Äì40)**

### **31. Classification models include:**

-  SVM
    
-  Logistic Regression
    
-  Naive Bayes
    
-  PCA
    

---

### **32. Confusion matrix measures:**

-  TP
    
-  FP
    
-  Accuracy (derived)
    
-  Feature count
    

---

### **33. SVM kernels include:**

-  Linear
    
-  RBF
    
-  Polynomial
    
-  Entropy
    

---

### **34. K-NN characteristics:**

-  Distance-based
    
-  Lazy learning
    
-  Needs training
    
-  Stores data
    

---

### **35. Naive Bayes assumptions:**

-  Independence
    
-  Gaussian distribution (for Gaussian NB)
    
-  Correlated features
    
-  Priors
    

---

### **36. Decision tree criteria include:**

-  Gini
    
-  Entropy
    
-  Chi-square
    
-  Support
    

---

### **37. Random Forest benefits:**

-  Handles missing data
    
-  Handles nonlinear data
    
-  Single deep tree
    
-  Robust to overfitting
    

---

### **38. ROC curve shows:**

-  TPR
    
-  FPR
    
-  Precision
    
-  Recall
    

---

### **39. Evaluation metrics include:**

-  F1-score
    
-  Precision
    
-  MAE
    
-  Recall
    

---

### **40. Logistic Regression uses:**

-  Sigmoid
    
-  Linear decision boundary
    
-  Multi-class variants
    
-  PCA
    

---

# **Unit 5 ‚Äî Checkbox Questions (41‚Äì50)**

### **41. Overfitting causes:**

-  High training accuracy
    
-  Low test accuracy
    
-  Good generalization
    
-  Model complexity
    

---

### **42. Underfitting causes:**

-  Simple model
    
-  Poor training performance
    
-  Poor test performance
    
-  Too many parameters
    

---

### **43. Cross-validation types include:**

-  k-fold
    
-  Leave-one-out
    
-  Bootstrap testing
    
-  Sequential sampling
    

---

### **44. Performance measures include:**

-  Confusion matrix
    
-  ROC curve
    
-  Loss function
    
-  Tree depth
    

---

### **45. Bias‚Äìvariance tradeoff statements:**

-  High bias = underfitting
    
-  High variance = overfitting
    
-  Balance required
    
-  Not relevant
    

---

### **46. Bootstrapping uses:**

-  Sampling with replacement
    
-  Bagging
    
-  Feature selection
    
-  Data simulation
    

---

### **47. Confusion matrix terms include:**

-  True Positive
    
-  False Positive
    
-  True Negative
    
-  Mean Squared Error
    

---

### **48. ROC shows:**

-  Threshold performance
    
-  Sensitivity
    
-  Specificity
    
-  Variance
    

---

### **49. Methods to reduce overfitting:**

-  Regularization
    
-  Dropout
    
-  More noise
    
-  Collect more data
    

---

### **50. Resampling techniques include:**

-  k-fold CV
    
-  Bootstrapping
    
-  Ensemble bagging
    
-  PCA
    

---

# **üìê Calculation-Based MCQs (All Included)**

### **51. Information Gain calculation:**

**Answer:** 0.120 bits (closest option)

---

### **52. Naive Bayes prediction (word = "free"):**

**Answer:** Spam

---

### **53. Logistic Regression probability (x = ‚àí4):**

**Answer:** 0.5

---

### **54. Gini Index calculation (3 Yes, 1 No):**

**Answer:** 0.375

---

### **55. Naive Bayes multi-feature example:**

**Answer:** Yes = 0.18, No = 0.02

---

### **56. Entropy calculation (2 A, 3 B):**

**Answer:** 0.918 (closest)

---

### **57. KNN distance (test ‚Üí choose nearest):**

**Answer:** Red

---

### **58. Logistic Regression multi-feature example:**

**Answer:** ‚âà0.60 ‚Üí closest option 0.69

---

### **59. Information Gain binary split:**

**Answer:** 0.05

---

### **60. Which SVM parameter controls margin vs misclassification?**

**Answer:** C

---

### **61. Increasing gamma in RBF SVM:**

**Answer:** More complex curved boundary

---

### **62. Increasing k in KNN:**

**Answer:** Lower variance

---

### **63. Minkowski p=1 equals:**

**Answer:** Manhattan distance

---

### **64. Very small k (e.g., 1):**

**Answer:** High variance

---

### **65. SVM parameter controlling support vector influence:**

**Answer:** gamma
.